{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI Analysis: Explaining Higgs Boson Predictions\n",
    "\n",
    "This notebook applies various Explainable AI (XAI) methods to understand model predictions:\n",
    "- SHAP (SHapley Additive exPlanations)\n",
    "- LIME (Local Interpretable Model-agnostic Explanations)\n",
    "- Integrated Gradients\n",
    "- DeepLIFT\n",
    "- Gradient SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from data_loader import load_higgs_data, get_feature_names\n",
    "from models import create_model\n",
    "from xai_methods import XAIAnalyzer\n",
    "from visualization import plot_attribution_heatmap\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'model_type': 'simple',\n",
    "    'model_path': '../models/higgs_classifier_simple.pth',\n",
    "    'n_test_samples': 500  # Number of test samples to analyze\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train, X_test, y_train, y_test = load_higgs_data(\n",
    "    data_path='../data/HIGGS.csv',\n",
    "    n_samples=50000,\n",
    "    test_split=0.2,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "feature_names = get_feature_names()\n",
    "\n",
    "# Use subset for faster XAI computation\n",
    "X_test_subset = X_test[:config['n_test_samples']]\n",
    "y_test_subset = y_test[:config['n_test_samples']]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples for XAI: {len(X_test_subset)}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "input_dim = X_train.shape[1]\n",
    "model = create_model(\n",
    "    model_type=config['model_type'],\n",
    "    input_dim=input_dim\n",
    ")\n",
    "\n",
    "# Try to load trained weights, otherwise use untrained model\n",
    "try:\n",
    "    model.load_state_dict(torch.load(config['model_path'], map_location=device))\n",
    "    print(f\"Loaded model from {config['model_path']}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Model file not found at {config['model_path']}\")\n",
    "    print(\"Using untrained model for demonstration purposes\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize XAI Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XAI analyzer\n",
    "xai_analyzer = XAIAnalyzer(\n",
    "    model=model,\n",
    "    feature_names=feature_names,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"XAI Analyzer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SHAP Analysis\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) provides a unified measure of feature importance based on game theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing SHAP values...\")\n",
    "shap_values, shap_explainer = xai_analyzer.compute_shap_values(\n",
    "    X_background=X_train,\n",
    "    X_test=X_test_subset,\n",
    "    n_background=100\n",
    ")\n",
    "\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP summary\n",
    "xai_analyzer.plot_shap_summary(\n",
    "    shap_values,\n",
    "    X_test_subset,\n",
    "    max_display=20,\n",
    "    save_path='../figures/shap_summary.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance from SHAP\n",
    "xai_analyzer.plot_feature_importance(\n",
    "    shap_values,\n",
    "    top_k=15,\n",
    "    title='Feature Importance (SHAP)',\n",
    "    save_path='../figures/shap_importance.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LIME Analysis\n",
    "\n",
    "LIME explains individual predictions by learning an interpretable model locally around the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few interesting examples\n",
    "# Get signal and background examples\n",
    "signal_idx = np.where(y_test_subset == 1)[0][0]\n",
    "background_idx = np.where(y_test_subset == 0)[0][0]\n",
    "\n",
    "print(f\"Analyzing signal example at index {signal_idx}\")\n",
    "lime_exp_signal = xai_analyzer.compute_lime_explanation(\n",
    "    X_train=X_train,\n",
    "    X_instance=X_test_subset[signal_idx],\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "print(\"\\nTop features for signal prediction:\")\n",
    "for feature, weight in lime_exp_signal['feature_weights']:\n",
    "    print(f\"  {feature}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nAnalyzing background example at index {background_idx}\")\n",
    "lime_exp_background = xai_analyzer.compute_lime_explanation(\n",
    "    X_train=X_train,\n",
    "    X_instance=X_test_subset[background_idx],\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "print(\"\\nTop features for background prediction:\")\n",
    "for feature, weight in lime_exp_background['feature_weights']:\n",
    "    print(f\"  {feature}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integrated Gradients\n",
    "\n",
    "Integrated Gradients attributes the prediction to input features by integrating gradients along a path from a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing Integrated Gradients...\")\n",
    "ig_attributions = xai_analyzer.compute_integrated_gradients(\n",
    "    X_test=X_test_subset,\n",
    "    n_steps=50\n",
    ")\n",
    "\n",
    "print(f\"IG attributions shape: {ig_attributions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance from Integrated Gradients\n",
    "xai_analyzer.plot_feature_importance(\n",
    "    ig_attributions,\n",
    "    top_k=15,\n",
    "    title='Feature Importance (Integrated Gradients)',\n",
    "    save_path='../figures/ig_importance.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DeepLIFT\n",
    "\n",
    "DeepLIFT compares the activation of each neuron to its reference activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing DeepLIFT attributions...\")\n",
    "deeplift_attributions = xai_analyzer.compute_deeplift(\n",
    "    X_test=X_test_subset\n",
    ")\n",
    "\n",
    "print(f\"DeepLIFT attributions shape: {deeplift_attributions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance from DeepLIFT\n",
    "xai_analyzer.plot_feature_importance(\n",
    "    deeplift_attributions,\n",
    "    top_k=15,\n",
    "    title='Feature Importance (DeepLIFT)',\n",
    "    save_path='../figures/deeplift_importance.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare XAI Methods\n",
    "\n",
    "Compare all XAI methods side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare methods\n",
    "comparison_results = xai_analyzer.compare_methods(\n",
    "    X_background=X_train,\n",
    "    X_test=X_test_subset[:100],  # Use smaller subset for faster computation\n",
    "    methods=['shap', 'ig', 'deeplift'],\n",
    "    save_path='../figures/xai_comparison.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Attribution Heatmap\n",
    "\n",
    "Visualize attributions across multiple samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot attribution heatmap for SHAP values\n",
    "plot_attribution_heatmap(\n",
    "    shap_values[:50],  # First 50 samples\n",
    "    feature_names,\n",
    "    n_samples=20,\n",
    "    save_path='../figures/attribution_heatmap.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Ranking\n",
    "\n",
    "Compare feature rankings across different XAI methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate mean absolute attributions for each method\n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "ig_importance = np.abs(ig_attributions).mean(axis=0)\n",
    "deeplift_importance = np.abs(deeplift_attributions).mean(axis=0)\n",
    "\n",
    "# Create DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'SHAP': shap_importance,\n",
    "    'Integrated Gradients': ig_importance,\n",
    "    'DeepLIFT': deeplift_importance\n",
    "})\n",
    "\n",
    "# Sort by SHAP importance\n",
    "importance_df = importance_df.sort_values('SHAP', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features:\")\n",
    "print(importance_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated various XAI methods for explaining Higgs boson classification:\n",
    "\n",
    "1. **SHAP**: Provides global feature importance and individual explanations\n",
    "2. **LIME**: Explains individual predictions with local interpretable models\n",
    "3. **Integrated Gradients**: Attributes predictions through gradient integration\n",
    "4. **DeepLIFT**: Compares activations to reference values\n",
    "\n",
    "All methods help understand which features are most important for distinguishing signal from background events."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
